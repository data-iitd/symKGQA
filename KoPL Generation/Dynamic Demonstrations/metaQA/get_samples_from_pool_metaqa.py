import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import torch
from datetime import datetime
import json
import random

# hop2_samples = [(11189, 1725), (7480, 1674), (33699, 1512), (20486, 1461), (27190, 1458), (37349, 1404), (26035, 1402), (76127, 1385), (47791, 1372), (41119, 1360), (1708, 1327), (36097, 1327), (15342, 1317), (41617, 1316), (74003, 1314), (50902, 1311), (17768, 1298), (29771, 1298), (67970, 1281), (22191, 1265), (13496, 1265), (74813, 1262), (53264, 1223), (40894, 1208), (19063, 1206), (41888, 1199), (65402, 1191), (30212, 1191), (16754, 1184), (63861, 1181), (27227, 1171), (30483, 1169), (35978, 1163), (72681, 1155), (36339, 1148), (33679, 1133), (51212, 1125), (23473, 1122), (8678, 1122), (28594, 1120), (78663, 1115), (27680, 1103), (4365, 1102), (67907, 1091), (29482, 1088), (25815, 1088), (55436, 1082), (50189, 1082), (22925, 1081), (36904, 1080), (70677, 1072), (32399, 1067), (17942, 1062), (38910, 1054), (59957, 1046), (43124, 1046), (2523, 1045), (47001, 1045), (69747, 1044), (75625, 1042), (43257, 1039), (21074, 1037), (61482, 1037), (26216, 1034), (36194, 1034), (24775, 1034), (2348, 1030), (19446, 1024), (17589, 1024), (68192, 1023), (77178, 1016), (62023, 1014), (57627, 1013), (60562, 1009), (40475, 1006), (59612, 1006), (59408, 1001), (3013, 1001), (50332, 1000), (31816, 998), (26108, 995), (71240, 995), (10420, 993), (15989, 993), (44749, 993), (68204, 991), (43176, 990), (37151, 985), (46885, 984), (17995, 983), (25817, 983), (42029, 982), (37253, 980), (60605, 979), (59197, 979), (12391, 978), (59312, 978), (31104, 977), (64579, 977), (63226, 976)]
hop3_samples = [(26841, 3473), (8007, 3096), (25624, 3071), (21428, 2913), (19492, 2730), (5129, 2609), (14015, 2597), (31691, 2585), (59886, 2576), (63987, 2514), (54465, 2468), (31823, 2451), (55753, 2444), (3291, 2276), (47071, 2265), (68588, 2214), (34957, 2180), (61106, 2175), (8626, 2173), (39737, 2170), (5993, 2165), (58319, 2103), (22808, 2102), (28902, 2097), (30525, 2094), (55537, 2055), (35616, 2051), (58548, 2030), (54813, 2015), (39088, 1994), (72900, 1984), (37466, 1951), (32538, 1906), (40817, 1895), (4353, 1874), (46385, 1871), (33754, 1852), (76200, 1845), (10532, 1844), (24307, 1842), (10478, 1835), (33675, 1819), (50244, 1815), (64453, 1813), (44897, 1799), (63166, 1785), (8339, 1768), (32573, 1752), (1453, 1752), (12227, 1745), (47533, 1726), (71706, 1723), (26626, 1720), (55693, 1718), (37928, 1717), (369, 1717), (62672, 1715), (55822, 1715), (76710, 1714), (74494, 1714), (4025, 1705), (76402, 1702), (33363, 1701), (43116, 1697), (17848, 1692), (58316, 1678), (72150, 1674), (41329, 1671), (59812, 1659), (10987, 1657), (46330, 1652), (41879, 1631), (38315, 1627), (47094, 1625), (76821, 1611), (497, 1606), (74679, 1601), (4276, 1594), (67138, 1593), (60891, 1585), (22560, 1583), (2789, 1582), (64238, 1578), (52193, 1577), (57660, 1571), (63337, 1571), (39798, 1563), (11013, 1562), (72172, 1550), (63392, 1545), (3382, 1539), (59263, 1533), (41161, 1532), (18573, 1531), (51615, 1530), (43869, 1529), (64386, 1528), (67806, 1524), (19921, 1522), (37723, 1519)]
# hop1_samples = [19313, 3017, 17100, 14943, 14394, 17651, 15166, 16312, 1316, 16778, 13278, 37058, 4849, 40655, 37175, 4470, 6121, 8165, 12432, 10697, 38408, 10079, 28978, 27643, 29000, 22065, 27086, 24561, 33400, 28311, 24762, 22412, 28286, 34500, 34637, 34553, 34423, 34505, 34448, 34610, 34659, 34674, 34636, 34602, 45633, 45632, 45691, 45695, 45620, 45688, 45692, 45629, 45604, 45637, 45597, 48073, 46386, 47018, 48182, 46410, 46848, 47565, 46706, 47059, 46582, 46459, 48927, 57914, 55010, 51277, 53274, 56500, 52294, 48895, 56333, 58216, 57793, 75019, 83940, 78271, 83520, 82606, 77156, 85871, 76030, 82864, 82535, 61570, 93805, 73885, 95668, 95654, 68948, 74023, 95009, 73773, 65845, 67528, 65097]

model = SentenceTransformer('all-distilroberta-v1')
# setup Chroma in-memory, for easy prototyping. Can add persistence easily!
client = chromadb.PersistentClient(path="./")

f = open('../data/metaQA/1-hop/vanilla/qa_test.txt')
test_data = f.readlines()
f.close()

f = open('../data/metaQA/1-hop/vanilla/qa_train.txt')
train_data = f.readlines()
f.close()

file = open('../data/metaQA/metaqa_train.json')
annotated_data = json.load(file)
file.close()

annotated_data_dict = {}
relation_dict = {}
for d in annotated_data:
    q = d['nlq']
    p = d['kopl'].replace('QueryAttr', 'Relate').split(').')
    relation = p[1].replace('Relate(', '').split(',')[0].replace(')', '')
    annotated_data_dict[q] = p

train_samples = []
for i,_ in hop3_samples:
    train_samples.append(train_data[i].split('\t')[0].replace('[', '').replace(']', ''))


collection = client.get_or_create_collection("top-100-documents-metaqa3hop-new", metadata={"hnsw:space": "cosine"})
collection.add(
        embeddings=[element.tolist() for element in sentence_embeddings],
        ids=[str(i) for i,_ in hop3_samples], # unique for each doc
    )

def create_icl(ques_ids, train_data, annotated_data_dict):
    ques_prompt = ""
    i = 1
    for ques_id in ques_ids:
        ques = train_data[int(ques_id)].split('\t')[0]
        entity = ques[ques.find("[")+1:ques.find("]")]
        ques = ques.replace('[', '').replace(']', '')
        program = annotated_data_dict[ques]
        ques_prompt += f"Training Example {i}:\nQuestion: {ques}. Entities: ['{entity}']. The steps to solve this question are:\nOutput:\n"
        for j in range(len(program)):
            if program[j].endswith(')'):
                ques_prompt += f"Step# {j+1}: {program[j]}\n"
            else:
                ques_prompt += f"Step# {j+1}: {program[j]})\n"
        ques_prompt += "Done\n\n"
        i += 1
    ques_prompt += "Test Example:\nQuestion: "
    return ques_prompt
